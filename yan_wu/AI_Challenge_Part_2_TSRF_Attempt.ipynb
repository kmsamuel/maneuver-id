{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI Challenge Part 2 TSRF Attempt.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**About**\n",
        "\n",
        "This is a Time-series Random Forest approach on identifying the type of maneuver for a given dataset. The package sktime is mainly used to train the model."
      ],
      "metadata": {
        "id": "fLbSAMPH3VZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sktime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoJhsQM6Q8yk",
        "outputId": "e49bdfc9-ae08-40c0-b5dd-6ee30b0624c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sktime\n",
            "  Downloading sktime-0.11.2-py3-none-any.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting deprecated>=1.2.13\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting numba>=0.53\n",
            "  Downloading numba-0.55.1-1-cp37-cp37m-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 43.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas<1.5.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.0.2)\n",
            "Requirement already satisfied: numpy<1.22,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.21.6)\n",
            "Collecting statsmodels>=0.12.1\n",
            "  Downloading statsmodels-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 27.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy<1.8.0 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.4.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.13->sktime) (1.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.53->sktime) (57.4.0)\n",
            "Collecting llvmlite<0.39,>=0.38.0rc1\n",
            "  Downloading llvmlite-0.38.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 34.5 MB 9.5 kB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<1.5.0,>=1.1.0->sktime) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<1.5.0,>=1.1.0->sktime) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas<1.5.0,>=1.1.0->sktime) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.0->sktime) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.0->sktime) (3.1.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.12.1->sktime) (21.3)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.12.1->sktime) (0.5.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=21.3->statsmodels>=0.12.1->sktime) (3.0.8)\n",
            "Installing collected packages: llvmlite, statsmodels, numba, deprecated, sktime\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "Successfully installed deprecated-1.2.13 llvmlite-0.38.0 numba-0.55.1 sktime-0.11.2 statsmodels-0.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uR7gO_5K2YIY"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sktime\n",
        "import math\n",
        "# from sktime.classification.kernel_based import TimeSeriesForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the display option\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "metadata": {
        "id": "PvBnCHxB6bwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Engineering**\n",
        "\n",
        "The datasets were resampled to contain 800 entries (the average length of the datasets), and the NaN values were filled in based on the nearby values."
      ],
      "metadata": {
        "id": "GTU44tNo6BZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_lcm(x, y):\n",
        "  \"\"\"\n",
        "  A helper function to compute the least common multiple of two integers.\n",
        "\n",
        "  Input:\n",
        "  x: an integer\n",
        "  y: an integer\n",
        "\n",
        "  Output:\n",
        "  The least common multiple of two input integers.\n",
        "  \"\"\"\n",
        "  if x > y:\n",
        "      greater = x\n",
        "  else:\n",
        "      greater = y\n",
        "\n",
        "  while(True):\n",
        "      if((greater % x == 0) and (greater % y == 0)):\n",
        "          lcm = greater\n",
        "          break\n",
        "      greater += 1\n",
        "  return lcm\n",
        "\n",
        "def downsample(x,q):\n",
        "    \"\"\"\n",
        "    A helper function to down-sample a given time-series dataset.\n",
        "\n",
        "    Input: \n",
        "    x: Time-series data\n",
        "    q: The integer ratio that the sample needs to be reduced to.\n",
        "\n",
        "    Output:\n",
        "    The down-sampled time-series data.\n",
        "    \"\"\"\n",
        "    if type(q) == float or q <= 0:\n",
        "        return []\n",
        "    else:\n",
        "        result = []\n",
        "        for i in range(len(x)):\n",
        "            if i%q == 0:\n",
        "                result.append(x[i])\n",
        "        return result\n",
        "\n",
        "def upsample(x,p):\n",
        "    \"\"\"\n",
        "      A helper function to up-sample a given time-series dataset.\n",
        "\n",
        "      Input: \n",
        "      x: Time-series data\n",
        "      p: The integer ratio that the sample needs to be increased to.\n",
        "\n",
        "      Output:\n",
        "      The up-sampled time-series data.\n",
        "    \"\"\"\n",
        "    if type(p) == float or p <= 0:\n",
        "        return []\n",
        "    result = []\n",
        "    for i in range(len(x)):\n",
        "        result.append(x[i])\n",
        "        if i != len(x)-1:\n",
        "            for j in range(1, p):\n",
        "                result.append(x[i] + (x[i+1]-x[i]) *j / p) \n",
        "        else:\n",
        "            for j in range(1, p):\n",
        "                result.append(x[i] + j/p * (x[i] - x[i-1])) \n",
        "    return result\n",
        "\n",
        "def resample(inp,desired_length):\n",
        "    \"\"\"\n",
        "      A helper function to resample a given time-series dataset.\n",
        "\n",
        "      Input: \n",
        "      inp: Time-series data\n",
        "      desired_length: The number of entries that the sample needs to be resized to.\n",
        "\n",
        "      Output:\n",
        "      The resampled time-series data.\n",
        "    \"\"\"\n",
        "    l = compute_lcm(len(inp), desired_length)\n",
        "    result1 = upsample(inp, int(l/len(inp)))\n",
        "    result = downsample(result1, int(l/desired_length))\n",
        "    return result"
      ],
      "metadata": {
        "id": "4Mav4uZr0N2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_in_nan(array, i, j, row, col):\n",
        "    average = 0\n",
        "    count = 0\n",
        "    if i != 0 and not np.isnan(array[i-1][j]):\n",
        "      average += array[i-1][j]\n",
        "      count += 1\n",
        "    if i != row-1  and not np.isnan(array[i+1][j]):\n",
        "      average += array[i+1][j]\n",
        "      count += 1\n",
        "    if count >= 0:\n",
        "      return average/count"
      ],
      "metadata": {
        "id": "exAD1u_XzOvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read In Dataset**\n",
        "\n",
        "The dataset is cleaned such that the number of rows were resampled, the NaN values were filled in, and only relevant columns were included. The format of dataset is specified for the further model training."
      ],
      "metadata": {
        "id": "MC2wlLdd6qUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(file_locaton, all_files, train=False):\n",
        "    \"\"\"\n",
        "    Read in each dataset and convert them to a pandas dataframe.\n",
        "\n",
        "    Input: \n",
        "      file_location: the file directory for all the sample maneuvers.\n",
        "      all_files: the file names within the file directory.\n",
        "    \n",
        "    Output:\n",
        "      df: a pandas dataframe containing relevant columns \n",
        "    \"\"\"\n",
        "    if train:\n",
        "      factors = ['vx (m/s)', 'vy (m/s)', 'vz (m/s)', 'head (deg)', 'roll (deg)', 'pitch (deg)']\n",
        "    else:\n",
        "      factors = [' vx (m/s)', ' vy (m/s)', ' vz (m/s)', ' head (deg)', ' roll (deg)', ' pitch (deg)']\n",
        "    array = [[[] for _ in range(len(factors))] for _ in range(len(all_files))]\n",
        "    if train:\n",
        "      labels = []\n",
        "    for index in range(len(all_files)):\n",
        "      file = all_files[index]\n",
        "      df = pd.read_csv(file_location+\"/\"+file, sep='\\t')\n",
        "      df = df[factors]\n",
        "      df_array = df.to_numpy()\n",
        "      for i in range(len(df_array)):\n",
        "        for j in range(len(df_array[0])):\n",
        "          if np.isnan(df_array[i][j]):\n",
        "            fill_value = fill_in_nan(df_array, i, j, len(df_array), len(df_array[0]))\n",
        "            array[index][j].append(fill_value)\n",
        "          else:\n",
        "            array[index][j].append(df_array[i][j])\n",
        "      if train:\n",
        "        labels.append(file[11:-8])\n",
        "\n",
        "    for index in range(len(all_files)):\n",
        "      for l in range(len(array[index])):\n",
        "        array[index][l] = resample(array[index][l], 800) #800 is the average length of the training datasets\n",
        "        array[index][l] = pd.Series(array[index][l])\n",
        "    if train:\n",
        "      return pd.DataFrame(array, dtype=object), pd.Series(labels)\n",
        "    else:\n",
        "      return pd.DataFrame(array, dtype=object)\n"
      ],
      "metadata": {
        "id": "AJEfy-LA2spJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training**\n",
        "\n",
        "Train the multivariate model by concatenating the variables.\n",
        "Use the model to output the desired label."
      ],
      "metadata": {
        "id": "NiYi1nwN9squ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sktime.transformations.panel.compose import ColumnConcatenator\n",
        "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
        "from sktime.classification.compose import ColumnEnsembleClassifier\n",
        "from sktime.classification.dictionary_based import BOSSEnsemble\n",
        "\n",
        "file_location = \"/content/drive/MyDrive/VanceAFB_tsv\"\n",
        "X_train, y_train = read_data(file_location , os.listdir(\"/content/drive/MyDrive/VanceAFB_tsv\"), True)"
      ],
      "metadata": {
        "id": "Lr-SHp8WegI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = [\n",
        "    (\"concatenate\", ColumnConcatenator()),\n",
        "    (\"classify\", TimeSeriesForestClassifier(n_estimators=100)),\n",
        "]\n",
        "clf = Pipeline(steps)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "file_name = ['12000001002.min.tsv']\n",
        "file_location = \"/content/drive/MyDrive/12000000000_tsv_good\"\n",
        "X_test = read_data(file_location, file_name)\n",
        "y_pred = clf.predict(X_test)\n",
        "y_pred\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NbTJoExmniJ",
        "outputId": "558782ad-d3ff-44ff-bcfe-8cf9a9004a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['StraightIn'], dtype='<U10')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}